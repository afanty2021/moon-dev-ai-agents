# ä½¿ç”¨æ–¹æ³•

<cite>
**æœ¬æ–‡æ¡£ä¸­å¼•ç”¨çš„æ–‡ä»¶**
- [model_factory.py](file://src/models/model_factory.py)
- [claude_model.py](file://src/models/claude_model.py)
- [base_model.py](file://src/models/base_model.py)
- [base_agent.py](file://src/agents/base_agent.py)
- [chat_agent_ad.py](file://src/agents/chat_agent_ad.py)
- [rbi_agent.py](file://src/agents/rbi_agent.py)
- [focus_agent.py](file://src/agents/focus_agent.py)
- [risk_agent.py](file://src/agents/risk_agent.py)
- [chat_question_generator.py](file://src/agents/chat_question_generator.py)
- [polymarket_agent.py](file://src/agents/polymarket_agent.py)
- [config.py](file://src/config.py)
</cite>

## ç›®å½•
1. [ç®€ä»‹](#ç®€ä»‹)
2. [é¡¹ç›®æ¶æ„æ¦‚è§ˆ](#é¡¹ç›®æ¶æ„æ¦‚è§ˆ)
3. [ModelFactoryåŸºç¡€](#modelfactoryåŸºç¡€)
4. [Claudeæ¨¡å‹åˆå§‹åŒ–](#claudeæ¨¡å‹åˆå§‹åŒ–)
5. [æ¶ˆæ¯æ ¼å¼ä¸ç»“æ„](#æ¶ˆæ¯æ ¼å¼ä¸ç»“æ„)
6. [åŒæ­¥è°ƒç”¨ç¤ºä¾‹](#åŒæ­¥è°ƒç”¨ç¤ºä¾‹)
7. [å¼‚æ­¥è°ƒç”¨ä¸æµå¼å“åº”](#å¼‚æ­¥è°ƒç”¨ä¸æµå¼å“åº”)
8. [å¤šè½®å¯¹è¯ä¸ä¸Šä¸‹æ–‡ç®¡ç†](#å¤šè½®å¯¹è¯ä¸ä¸Šä¸‹æ–‡ç®¡ç†)
9. [é•¿æ–‡æœ¬ç”Ÿæˆå¤„ç†](#é•¿æ–‡æœ¬ç”Ÿæˆå¤„ç†)
10. [AIä»£ç†ç³»ç»Ÿé›†æˆ](#aiä»£ç†ç³»ç»Ÿé›†æˆ)
11. [æœ€ä½³å®è·µä¸ä¼˜åŒ–å»ºè®®](#æœ€ä½³å®è·µä¸ä¼˜åŒ–å»ºè®®)
12. [æ•…éšœæ’é™¤æŒ‡å—](#æ•…éšœæ’é™¤æŒ‡å—)

## ç®€ä»‹

æœ¬æ•™ç¨‹å±•ç¤ºäº†å¦‚ä½•åœ¨Moon Dev AIä»£ç†ç³»ç»Ÿä¸­ä½¿ç”¨Claudeæ¨¡å‹ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ç»Ÿä¸€çš„ModelFactoryæ¨¡å¼ç®¡ç†å¤šç§AIæ¨¡å‹ï¼Œä¸ºå¼€å‘è€…æä¾›äº†ç®€æ´è€Œå¼ºå¤§çš„æ¥å£æ¥é›†æˆClaudeæ¨¡å‹è¿›è¡Œå„ç§AIä»»åŠ¡ã€‚

Claudeæ¨¡å‹ä½œä¸ºAnthropicå…¬å¸å¼€å‘çš„å…ˆè¿›è¯­è¨€æ¨¡å‹ï¼Œåœ¨ä»£ç ç”Ÿæˆã€æ•°æ®åˆ†æã€å®æ—¶èŠå¤©ç­‰åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡æœ¬æ•™ç¨‹ï¼Œæ‚¨å°†å­¦ä¼šå¦‚ä½•ï¼š
- é€šè¿‡ModelFactoryè·å–Claudeæ¨¡å‹å®ä¾‹
- å®ç°åŒæ­¥å’Œå¼‚æ­¥è°ƒç”¨
- å¤„ç†å¤šè½®å¯¹è¯å’Œä¸Šä¸‹æ–‡ç®¡ç†
- ä¼˜åŒ–é•¿æ–‡æœ¬ç”Ÿæˆ
- åœ¨AIä»£ç†ç³»ç»Ÿä¸­é›†æˆClaudeæ¨¡å‹

## é¡¹ç›®æ¶æ„æ¦‚è§ˆ

Moon Dev AIä»£ç†ç³»ç»Ÿé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ï¼š

```mermaid
graph TB
subgraph "æ¨¡å‹å±‚"
MF[ModelFactory]
CM[ClaudeModel]
BM[BaseModel]
end
subgraph "ä»£ç†å±‚"
BA[BaseAgent]
CA[ChatAgent]
FA[FocusAgent]
RA[RiskAgent]
end
subgraph "é…ç½®å±‚"
CFG[Config]
ENV[Environment Variables]
end
MF --> CM
CM --> BM
BA --> MF
CA --> BA
FA --> BA
RA --> BA
CFG --> MF
ENV --> MF
```

**å›¾è¡¨æ¥æº**
- [model_factory.py](file://src/models/model_factory.py#L1-L50)
- [claude_model.py](file://src/models/claude_model.py#L1-L30)
- [base_agent.py](file://src/agents/base_agent.py#L1-L20)

**ç« èŠ‚æ¥æº**
- [model_factory.py](file://src/models/model_factory.py#L1-L261)
- [base_model.py](file://src/models/base_model.py#L1-L73)

## ModelFactoryåŸºç¡€

ModelFactoryæ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒå·¥å‚ç±»ï¼Œè´Ÿè´£ç®¡ç†å’Œåˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„AIæ¨¡å‹ã€‚å®ƒæä¾›äº†ç»Ÿä¸€çš„æ¥å£æ¥è·å–ç‰¹å®šç±»å‹çš„æ¨¡å‹å®ä¾‹ã€‚

### æ ¸å¿ƒç‰¹æ€§

ModelFactoryå…·å¤‡ä»¥ä¸‹å…³é”®ç‰¹æ€§ï¼š

| ç‰¹æ€§ | æè¿° | é»˜è®¤å€¼ |
|------|------|--------|
| æ¨¡å‹ç±»å‹æ˜ å°„ | å°†å­—ç¬¦ä¸²æ ‡è¯†ç¬¦æ˜ å°„åˆ°å…·ä½“å®ç°ç±» | `{"claude": ClaudeModel}` |
| é»˜è®¤æ¨¡å‹é…ç½® | ä¸ºæ¯ç§æ¨¡å‹ç±»å‹æä¾›é»˜è®¤é…ç½® | `{"claude": "claude-3-5-haiku-latest"}` |
| ç¯å¢ƒå˜é‡åŠ è½½ | è‡ªåŠ¨ä».envæ–‡ä»¶åŠ è½½APIå¯†é’¥ | æ”¯æŒå¤šç§æ¨¡å‹æä¾›å•† |
| å¯ç”¨æ€§æ£€æµ‹ | è‡ªåŠ¨æ£€æµ‹æ¨¡å‹æ˜¯å¦å¯è®¿é—® | å¥åº·æ£€æŸ¥æœºåˆ¶ |

### åˆå§‹åŒ–æµç¨‹

```mermaid
flowchart TD
Start([å¼€å§‹åˆå§‹åŒ–]) --> LoadEnv[åŠ è½½ç¯å¢ƒå˜é‡]
LoadEnv --> CheckKeys{æ£€æŸ¥APIå¯†é’¥}
CheckKeys --> |å­˜åœ¨| InitModels[åˆå§‹åŒ–æ¨¡å‹]
CheckKeys --> |ä¸å­˜åœ¨| LogWarning[è®°å½•è­¦å‘Š]
InitModels --> TestAvailability[æµ‹è¯•å¯ç”¨æ€§]
TestAvailability --> |æˆåŠŸ| RegisterModel[æ³¨å†Œæ¨¡å‹]
TestAvailability --> |å¤±è´¥| SkipModel[è·³è¿‡æ¨¡å‹]
RegisterModel --> MoreModels{è¿˜æœ‰æ›´å¤šæ¨¡å‹?}
MoreModels --> |æ˜¯| InitModels
MoreModels --> |å¦| Complete[åˆå§‹åŒ–å®Œæˆ]
LogWarning --> MoreModels
SkipModel --> MoreModels
Complete --> End([ç»“æŸ])
```

**å›¾è¡¨æ¥æº**
- [model_factory.py](file://src/models/model_factory.py#L60-L120)

**ç« èŠ‚æ¥æº**
- [model_factory.py](file://src/models/model_factory.py#L40-L150)

## Claudeæ¨¡å‹åˆå§‹åŒ–

### åŸºç¡€åˆå§‹åŒ–

è¦ä½¿ç”¨Claudeæ¨¡å‹ï¼Œé¦–å…ˆéœ€è¦é€šè¿‡ModelFactoryè·å–æ¨¡å‹å®ä¾‹ï¼š

```python
# è·å–Claudeæ¨¡å‹å®ä¾‹
model = model_factory.get_model("claude", "claude-3-5-haiku-latest")
```

### æ¨¡å‹é…ç½®é€‰é¡¹

Claudeæ¨¡å‹æ”¯æŒå¤šç§é…ç½®å‚æ•°ï¼š

| å‚æ•° | ç±»å‹ | æè¿° | é»˜è®¤å€¼ |
|------|------|------|--------|
| `model_name` | str | æŒ‡å®šä½¿ç”¨çš„Claudeæ¨¡å‹ç‰ˆæœ¬ | `"claude-3-5-haiku-latest"` |
| `temperature` | float | æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§å’Œåˆ›é€ æ€§ | `0.7` |
| `max_tokens` | int | æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•° | `1024` |
| `api_key` | str | Anthropic APIå¯†é’¥ | å¿…éœ€ |

### å¯ç”¨æ¨¡å‹åˆ—è¡¨

Claudeæ¨¡å‹æ”¯æŒä»¥ä¸‹ç‰ˆæœ¬ï¼š

| æ¨¡å‹åç§° | æ€§èƒ½ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|----------|----------|----------|
| `claude-3-5-haiku-latest` | å¿«é€Ÿé«˜æ•ˆ | æ—¥å¸¸å¯¹è¯ã€ç®€å•ä»»åŠ¡ |
| `claude-3-5-sonnet-latest` | å¹³è¡¡æ€§èƒ½ | ä¸­ç­‰å¤æ‚åº¦ä»»åŠ¡ |
| `claude-3-opus` | å¼ºå¤§æ¨ç†èƒ½åŠ› | å¤æ‚åˆ†æã€æ·±åº¦æ€è€ƒ |
| `claude-3-haiku` | è½»é‡å¿«é€Ÿ | å®æ—¶å“åº”ã€è½»é‡ä»»åŠ¡ |

**ç« èŠ‚æ¥æº**
- [claude_model.py](file://src/models/claude_model.py#L10-L30)
- [model_factory.py](file://src/models/model_factory.py#L40-L50)

## æ¶ˆæ¯æ ¼å¼ä¸ç»“æ„

Claudeæ¨¡å‹çš„æ¶ˆæ¯æ ¼å¼éµå¾ªæ ‡å‡†çš„OpenAIæ ¼å¼ï¼Œä½†å…·æœ‰ç‰¹å®šçš„ç³»ç»Ÿæç¤ºå’Œè§’è‰²å®šä¹‰ã€‚

### åŸºæœ¬æ¶ˆæ¯ç»“æ„

```python
messages = [
    {"role": "system", "content": "ç³»ç»Ÿæç¤ºè¯­"},
    {"role": "user", "content": "ç”¨æˆ·è¾“å…¥å†…å®¹"}
]
```

### æ¶ˆæ¯æ ¼å¼è§„èŒƒ

| è§’è‰² | å†…å®¹ç±»å‹ | ç”¨é€” | ç¤ºä¾‹ |
|------|----------|------|------|
| `system` | str | å®šä¹‰æ¨¡å‹è¡Œä¸ºå’Œä¸Šä¸‹æ–‡ | `"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹"` |
| `user` | str | ç”¨æˆ·è¾“å…¥çš„é—®é¢˜æˆ–æŒ‡ä»¤ | `"è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†"` |
| `assistant` | str | æ¨¡å‹çš„å›å¤å†…å®¹ | `"é‡å­è®¡ç®—æ˜¯ä¸€ç§..."` |

### ç³»ç»Ÿæç¤ºæœ€ä½³å®è·µ

æœ‰æ•ˆçš„ç³»ç»Ÿæç¤ºåº”è¯¥åŒ…å«ï¼š

1. **æ˜ç¡®çš„è§’è‰²å®šä¹‰**ï¼šæŒ‡å®šæ¨¡å‹çš„èº«ä»½å’Œä¸“ä¸šé¢†åŸŸ
2. **æ¸…æ™°çš„ä»»åŠ¡æè¿°**ï¼šè¯´æ˜æœŸæœ›çš„è¾“å‡ºæ ¼å¼å’Œè´¨é‡
3. **çº¦æŸæ¡ä»¶**ï¼šé™åˆ¶è¾“å‡ºèŒƒå›´å’Œé£æ ¼
4. **ç¤ºä¾‹æŒ‡å¯¼**ï¼šæä¾›æ ¼å¼å‚è€ƒ

**ç« èŠ‚æ¥æº**
- [claude_model.py](file://src/models/claude_model.py#L35-L55)
- [chat_agent_ad.py](file://src/agents/chat_agent_ad.py#L80-L120)

## åŒæ­¥è°ƒç”¨ç¤ºä¾‹

### åŸºç¡€åŒæ­¥è°ƒç”¨

æœ€ç®€å•çš„Claudeæ¨¡å‹è°ƒç”¨æ–¹å¼ï¼š

```python
# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = model_factory.get_model("claude", "claude-3-haiku-20240307")

# æ‰§è¡ŒåŒæ­¥è°ƒç”¨
response = model.generate_response(
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¨‹åºå‘˜",
    user_content="è¯·è§£é‡Šä»€ä¹ˆæ˜¯è£…é¥°å™¨",
    temperature=0.7,
    max_tokens=500
)

# è·å–ç»“æœ
result = response.content
```

### é«˜çº§åŒæ­¥è°ƒç”¨

å¯¹äºå¤æ‚çš„AIä»£ç†ä»»åŠ¡ï¼š

```python
# åœ¨ChatAgentä¸­çš„ä½¿ç”¨ç¤ºä¾‹
class ChatAgent:
    def __init__(self):
        self.model = model_factory.get_model("claude", "claude-3-haiku-20240307")
    
    def process_question(self, username, question):
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªç›´æ’­èŠå¤©åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·äº†è§£ç¼–ç¨‹å’Œäº¤æ˜“å†…å®¹ã€‚
        ç”¨æˆ·å: {username}
        å›ç­”åº”è¯¥å‹å¥½ã€ç®€æ´ï¼Œå¹¶åŒ…å«é€‚å½“çš„è¡¨æƒ…ç¬¦å·ã€‚"""
        
        response = self.model.generate_response(
            system_prompt=system_prompt,
            user_content=question,
            temperature=0.8,
            max_tokens=300
        )
        
        return response.content
```

### é”™è¯¯å¤„ç†æœºåˆ¶

```python
try:
    response = model.generate_response(
        system_prompt="ç³»ç»Ÿæç¤º",
        user_content="ç”¨æˆ·é—®é¢˜",
        temperature=0.7,
        max_tokens=1024
    )
    
    if response.content:
        return response.content
    else:
        raise ValueError("æ¨¡å‹è¿”å›ç©ºå†…å®¹")
        
except Exception as e:
    print(f"Claudeè°ƒç”¨å¤±è´¥: {e}")
    # å®ç°é‡è¯•é€»è¾‘æˆ–é™çº§ç­–ç•¥
```

**ç« èŠ‚æ¥æº**
- [claude_model.py](file://src/models/claude_model.py#L35-L75)
- [chat_agent_ad.py](file://src/agents/chat_agent_ad.py#L150-L200)

## å¼‚æ­¥è°ƒç”¨ä¸æµå¼å“åº”

### æµå¼å“åº”å¤„ç†

è™½ç„¶å½“å‰çš„Claudeæ¨¡å‹å®ç°ä¸»è¦æ”¯æŒåŒæ­¥è°ƒç”¨ï¼Œä½†ç³»ç»Ÿæ¶æ„æ”¯æŒæœªæ¥æ‰©å±•å¼‚æ­¥åŠŸèƒ½ï¼š

```python
# å¼‚æ­¥è°ƒç”¨æ¨¡æ¿ï¼ˆå¾…å®ç°ï¼‰
async def async_claude_call(model, system_prompt, user_content):
    try:
        # å¼‚æ­¥è°ƒç”¨é€»è¾‘
        response = await model.async_generate_response(
            system_prompt=system_prompt,
            user_content=user_content,
            temperature=0.7,
            max_tokens=1024
        )
        
        return response.content
        
    except Exception as e:
        print(f"å¼‚æ­¥è°ƒç”¨å¤±è´¥: {e}")
        return None
```

### ä¸Šä¸‹æ–‡æµå¤„ç†

å¯¹äºé•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡ï¼Œå¯ä»¥å®ç°æµå¼å¤„ç†ï¼š

```python
def stream_claude_response(model, system_prompt, user_content):
    """æ¨¡æ‹Ÿæµå¼å“åº”å¤„ç†"""
    try:
        # åˆ†å—å¤„ç†é•¿æ–‡æœ¬
        chunks = []
        remaining_content = user_content
        
        while remaining_content:
            # æ¯æ¬¡å¤„ç†å›ºå®šé•¿åº¦
            chunk_size = 1000
            chunk = remaining_content[:chunk_size]
            remaining_content = remaining_content[chunk_size:]
            
            # å¤„ç†å•ä¸ªå—
            response = model.generate_response(
                system_prompt=system_prompt,
                user_content=chunk,
                temperature=0.7,
                max_tokens=512
            )
            
            chunks.append(response.content)
            
            # æ¨¡æ‹Ÿå»¶è¿Ÿ
            time.sleep(0.1)
        
        return "".join(chunks)
        
    except Exception as e:
        print(f"æµå¼å¤„ç†å¤±è´¥: {e}")
        return None
```

**ç« èŠ‚æ¥æº**
- [base_model.py](file://src/models/base_model.py#L30-L50)

## å¤šè½®å¯¹è¯ä¸ä¸Šä¸‹æ–‡ç®¡ç†

### å¯¹è¯å†å²ç®¡ç†

åœ¨AIä»£ç†ç³»ç»Ÿä¸­ï¼Œæœ‰æ•ˆç®¡ç†å¯¹è¯ä¸Šä¸‹æ–‡è‡³å…³é‡è¦ï¼š

```python
class ConversationManager:
    def __init__(self, max_history=20):
        self.conversation_history = []
        self.max_history = max_history
    
    def add_message(self, role, content):
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å²"""
        self.conversation_history.append({"role": role, "content": content})
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.conversation_history) > self.max_history:
            self.conversation_history = self.conversation_history[-self.max_history:]
    
    def get_context(self):
        """è·å–å®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡"""
        return self.conversation_history.copy()
```

### ä¸Šä¸‹æ–‡å‹ç¼©æŠ€æœ¯

å¯¹äºé•¿å¯¹è¯ï¼Œå®æ–½ä¸Šä¸‹æ–‡å‹ç¼©ï¼š

```python
def compress_context(context, target_length=4000):
    """å‹ç¼©å¯¹è¯ä¸Šä¸‹æ–‡ä»¥æ§åˆ¶ä»¤ç‰Œä½¿ç”¨"""
    if len(str(context)) <= target_length:
        return context
    
    # ä¿ç•™æœ€è¿‘çš„é‡è¦æ¶ˆæ¯
    compressed = []
    important_roles = ["system", "user", "assistant"]
    
    for msg in reversed(context):
        if len(str(compressed)) + len(str(msg)) <= target_length:
            compressed.insert(0, msg)
        else:
            break
    
    return compressed
```

### å¤šAgentåä½œ

åœ¨å¤æ‚çš„AIç³»ç»Ÿä¸­ï¼Œå¤šä¸ªAgentå¯ä»¥å…±äº«ä¸Šä¸‹æ–‡ï¼š

```python
class MultiAgentCoordinator:
    def __init__(self):
        self.shared_context = []
        self.agents = {}
    
    def register_agent(self, agent_name, agent_instance):
        """æ³¨å†Œæ–°çš„AIä»£ç†"""
        self.agents[agent_name] = agent_instance
    
    def broadcast_message(self, message):
        """å‘æ‰€æœ‰ä»£ç†å¹¿æ’­æ¶ˆæ¯"""
        for agent_name, agent in self.agents.items():
            agent.process_shared_context(message)
```

**ç« èŠ‚æ¥æº**
- [chat_question_generator.py](file://src/agents/chat_question_generator.py#L90-L130)
- [polymarket_agent.py](file://src/agents/polymarket_agent.py#L730-L750)

## é•¿æ–‡æœ¬ç”Ÿæˆå¤„ç†

### æ–‡æœ¬åˆ†å—ç­–ç•¥

å¤„ç†é•¿æ–‡æœ¬ç”Ÿæˆæ—¶ï¼Œé‡‡ç”¨æ™ºèƒ½åˆ†å—ç­–ç•¥ï¼š

```python
def generate_long_text(model, system_prompt, base_content, chunk_size=2000, max_chunks=5):
    """ç”Ÿæˆé•¿æ–‡æœ¬å†…å®¹"""
    generated_chunks = []
    
    for chunk_num in range(max_chunks):
        # æ„å»ºå½“å‰å—çš„æç¤º
        chunk_prompt = f"""{system_prompt}
        
        ç»§ç»­å®Œæˆä»¥ä¸‹å†…å®¹çš„ç¬¬{chunk_num + 1}éƒ¨åˆ†ï¼ˆæ€»å…±æœ‰{max_chunks}éƒ¨åˆ†ï¼‰ï¼š
        
        {base_content}
        
        å¼€å§‹ç¬¬{chunk_num + 1}éƒ¨åˆ†ï¼š"""
        
        try:
            response = model.generate_response(
                system_prompt=chunk_prompt,
                user_content="",
                temperature=0.7,
                max_tokens=chunk_size // 2
            )
            
            if response.content:
                generated_chunks.append(response.content)
                base_content += response.content
            else:
                break
                
        except Exception as e:
            print(f"ç”Ÿæˆç¬¬{chunk_num + 1}éƒ¨åˆ†å¤±è´¥: {e}")
            break
    
    return "".join(generated_chunks)
```

### å†…å®¹è´¨é‡æ§åˆ¶

å®æ–½å¤šå±‚è´¨é‡æ£€æŸ¥ï¼š

```python
def quality_control(text, min_length=100):
    """æ£€æŸ¥ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡"""
    if not text or len(text.strip()) < min_length:
        return False, "æ–‡æœ¬è¿‡çŸ­"
    
    # æ£€æŸ¥é‡å¤å†…å®¹
    if len(set(text.split())) / len(text.split()) < 0.3:
        return False, "å¯èƒ½å­˜åœ¨é‡å¤å†…å®¹"
    
    # æ£€æŸ¥è¯­æ³•é”™è¯¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
    if text.count(".") < 1 or text.count("?") + text.count("!") < 1:
        return False, "å¯èƒ½ç¼ºå°‘æ ‡ç‚¹ç¬¦å·"
    
    return True, "è´¨é‡åˆæ ¼"
```

### ç¼–è¾‘ä¸ä¼˜åŒ–

å¯¹ç”Ÿæˆçš„é•¿æ–‡æœ¬è¿›è¡Œåå¤„ç†ï¼š

```python
def post_process_text(text):
    """åå¤„ç†ç”Ÿæˆçš„æ–‡æœ¬"""
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text).strip()
    
    # æ ¼å¼åŒ–æ®µè½
    paragraphs = text.split('\n')
    formatted_paragraphs = []
    
    for para in paragraphs:
        if para.strip():
            formatted_paragraphs.append(para.strip())
    
    return '\n\n'.join(formatted_paragraphs)
```

**ç« èŠ‚æ¥æº**
- [focus_agent.py](file://src/agents/focus_agent.py#L150-L200)
- [risk_agent.py](file://src/agents/risk_agent.py#L200-L250)

## AIä»£ç†ç³»ç»Ÿé›†æˆ

### åŸºç¡€Agentæ¶æ„

æ‰€æœ‰AIä»£ç†éƒ½ç»§æ‰¿è‡ªBaseAgentï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£ï¼š

```python
class BaseAgent:
    def __init__(self, agent_type, use_exchange_manager=False):
        self.type = agent_type
        self.start_time = datetime.now()
        self.em = None
        
        if use_exchange_manager:
            try:
                from src.exchange_manager import ExchangeManager
                self.em = ExchangeManager()
            except Exception:
                # å›é€€åˆ°ç›´æ¥å¯¼å…¥
                from src import nice_funcs as n
                self.n = n
```

### Claudeåœ¨ä¸åŒAgentä¸­çš„åº”ç”¨

#### èŠå¤©Agenté›†æˆ

```python
class ChatAgent(BaseAgent):
    def __init__(self):
        super().__init__("chat", use_exchange_manager=True)
        
        # é…ç½®Claudeæ¨¡å‹
        self.model = model_factory.get_model("claude", "claude-3-haiku-20240307")
        
        # è®¾ç½®å¯¹è¯å‚æ•°
        self.temperature = 0.8
        self.max_tokens = 300
        self.chat_memory_size = 30
    
    def process_question(self, username, question):
        """å¤„ç†ç”¨æˆ·é—®é¢˜"""
        system_prompt = self._build_system_prompt(username)
        
        response = self.model.generate_response(
            system_prompt=system_prompt,
            user_content=question,
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
        
        return response.content
```

#### é£é™©ç®¡ç†Agent

```python
class RiskAgent(BaseAgent):
    def __init__(self):
        super().__init__("risk")
        
        # åˆå§‹åŒ–Claudeå®¢æˆ·ç«¯
        anthropic_key = os.getenv("ANTHROPIC_KEY")
        self.client = Anthropic(api_key=anthropic_key)
        
        # é…ç½®AIåˆ†æå‚æ•°
        self.ai_model = "claude-3-5-sonnet-latest"
        self.ai_temperature = 0.7
        self.ai_max_tokens = 1024
    
    def analyze_market_risk(self, market_data):
        """ä½¿ç”¨Claudeåˆ†æå¸‚åœºé£é™©"""
        prompt = f"""åˆ†æä»¥ä¸‹å¸‚åœºæ•°æ®çš„é£é™©çŠ¶å†µï¼š
        
        æ•°æ®: {market_data}
        
        è¯·è¯„ä¼°:
        1. ä¸»è¦é£é™©å› ç´ 
        2. å»ºè®®çš„åº”å¯¹ç­–ç•¥
        3. é£é™©ç­‰çº§è¯„åˆ†ï¼ˆ1-10)"""
        
        message = self.client.messages.create(
            model=self.ai_model,
            max_tokens=self.ai_max_tokens,
            temperature=self.ai_temperature,
            system="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é£é™©ç®¡ç†é¡¾é—®",
            messages=[{"role": "user", "content": prompt}]
        )
        
        return message.content
```

#### ä¸“æ³¨åº¦åˆ†æAgent

```python
class FocusAgent(BaseAgent):
    def __init__(self):
        super().__init__("focus")
        
        # ä½¿ç”¨Claudeè¿›è¡Œä¸“æ³¨åº¦åˆ†æ
        self.model = model_factory.get_model("claude", "claude-3-haiku-20240307")
        
        self.focus_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“æ³¨åº¦åˆ†æä¸“å®¶ã€‚
        è¯·åˆ†æä»¥ä¸‹è½¬å½•å†…å®¹å¹¶ç»™å‡ºä¸“æ³¨åº¦è¯„åˆ†ã€‚
        
        è¿”å›æ ¼å¼:
        1. æ•°å­—è¯„åˆ†/10
        2. é¼“åŠ±æ€§è¯„è®º"""
    
    def analyze_focus(self, transcript):
        """åˆ†æä¸“æ³¨åº¦"""
        response = self.model.generate_response(
            system_prompt=self.focus_prompt,
            user_content=f"è½¬å½•å†…å®¹: {transcript}",
            temperature=0.5,
            max_tokens=100
        )
        
        return response.content
```

**ç« èŠ‚æ¥æº**
- [base_agent.py](file://src/agents/base_agent.py#L10-L58)
- [chat_agent_ad.py](file://src/agents/chat_agent_ad.py#L100-L150)
- [risk_agent.py](file://src/agents/risk_agent.py#L100-L150)
- [focus_agent.py](file://src/agents/focus_agent.py#L100-L200)

## æœ€ä½³å®è·µä¸ä¼˜åŒ–å»ºè®®

### æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### æ¨¡å‹é€‰æ‹©æŒ‡å—

| åœºæ™¯ | æ¨èæ¨¡å‹ | æ¸©åº¦è®¾ç½® | æœ€å¤§ä»¤ç‰Œæ•° |
|------|----------|----------|------------|
| å®æ—¶èŠå¤© | `claude-3-5-haiku-latest` | 0.8 | 300 |
| ä»£ç ç”Ÿæˆ | `claude-3-5-sonnet-latest` | 0.7 | 1024 |
| æ·±åº¦åˆ†æ | `claude-3-opus` | 0.6 | 2048 |
| ç®€å•é—®ç­” | `claude-3-haiku` | 0.5 | 200 |

#### ç¼“å­˜ç­–ç•¥

```python
class ClaudeCache:
    def __init__(self, ttl=300):  # 5åˆ†é’Ÿç¼“å­˜æ—¶é—´
        self.cache = {}
        self.ttl = ttl
    
    def get_cached_response(self, prompt_hash):
        """è·å–ç¼“å­˜çš„å“åº”"""
        if prompt_hash in self.cache:
            cached = self.cache[prompt_hash]
            if time.time() - cached['timestamp'] < self.ttl:
                return cached['response']
        return None
    
    def cache_response(self, prompt_hash, response):
        """ç¼“å­˜å“åº”"""
        self.cache[prompt_hash] = {
            'response': response,
            'timestamp': time.time()
        }
```

### é”™è¯¯å¤„ç†ä¸é‡è¯•

```python
def robust_claude_call(model, system_prompt, user_content, max_retries=3):
    """å¸¦é‡è¯•æœºåˆ¶çš„Claudeè°ƒç”¨"""
    for attempt in range(max_retries):
        try:
            response = model.generate_response(
                system_prompt=system_prompt,
                user_content=user_content,
                temperature=0.7,
                max_tokens=1024
            )
            
            if response.content:
                return response.content
                
        except Exception as e:
            if attempt == max_retries - 1:
                print(f"æœ€ç»ˆå°è¯•å¤±è´¥: {e}")
                return None
            
            wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
            time.sleep(wait_time)
    
    return None
```

### å®‰å…¨ä¸åˆè§„

#### è¾“å…¥éªŒè¯

```python
def validate_input(content, max_length=10000):
    """éªŒè¯ç”¨æˆ·è¾“å…¥çš„å®‰å…¨æ€§"""
    if not content or len(content.strip()) == 0:
        return False, "è¾“å…¥ä¸èƒ½ä¸ºç©º"
    
    if len(content) > max_length:
        return False, f"è¾“å…¥è¶…è¿‡æœ€å¤§é•¿åº¦é™åˆ¶({max_length}å­—ç¬¦)"
    
    # æ£€æŸ¥æ½œåœ¨çš„æ¶æ„å†…å®¹
    malicious_patterns = [
        r'<script.*?>.*?</script>',
        r'javascript:',
        r'data:text/',
        r'eval\('
    ]
    
    for pattern in malicious_patterns:
        if re.search(pattern, content, re.IGNORECASE):
            return False, "æ£€æµ‹åˆ°æ½œåœ¨çš„å®‰å…¨å¨èƒ"
    
    return True, "éªŒè¯é€šè¿‡"
```

#### è¾“å‡ºè¿‡æ»¤

```python
def filter_output(content):
    """è¿‡æ»¤æ•æ„Ÿä¿¡æ¯"""
    # ç§»é™¤å¯èƒ½çš„APIå¯†é’¥
    content = re.sub(r'[A-Za-z0-9]{40,}', '[REDACTED]', content)
    
    # ç§»é™¤ä¸ªäººèº«ä»½ä¿¡æ¯
    personal_info_patterns = [
        r'\b\d{3}-\d{2}-\d{4}\b',  # SSN
        r'\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b',  # é‚®ç®±
        r'\b\d{10,}\b'  # é•¿æ•°å­—åºåˆ—
    ]
    
    for pattern in personal_info_patterns:
        content = re.sub(pattern, '[PERSONAL_INFO]', content, flags=re.IGNORECASE)
    
    return content
```

**ç« èŠ‚æ¥æº**
- [config.py](file://src/config.py#L90-L110)
- [rbi_agent.py](file://src/agents/rbi_agent.py#L400-L500)

## æ•…éšœæ’é™¤æŒ‡å—

### å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

#### APIå¯†é’¥é—®é¢˜

```python
def diagnose_api_issues():
    """è¯Šæ–­APIç›¸å…³é—®é¢˜"""
    issues = []
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    anthropic_key = os.getenv("ANTHROPIC_KEY")
    if not anthropic_key:
        issues.append("âŒ ANTHROPIC_KEYæœªè®¾ç½®")
    elif len(anthropic_key) < 20:
        issues.append("âŒ ANTHROPIC_KEYæ ¼å¼ä¸æ­£ç¡®")
    
    # æ£€æŸ¥ç½‘ç»œè¿æ¥
    try:
        import requests
        response = requests.get("https://api.anthropic.com/v1/messages", timeout=5)
        if response.status_code != 401:
            issues.append(f"âš ï¸ APIç«¯ç‚¹å¯è¾¾ï¼Œä½†è¿”å›çŠ¶æ€ç : {response.status_code}")
    except requests.RequestException as e:
        issues.append(f"âš ï¸ æ— æ³•è¿æ¥åˆ°APIç«¯ç‚¹: {e}")
    
    return issues
```

#### æ¨¡å‹å¯ç”¨æ€§æ£€æŸ¥

```python
def check_model_availability(model_type="claude"):
    """æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§"""
    model = model_factory.get_model(model_type)
    
    if not model:
        print(f"âŒ æ— æ³•åˆå§‹åŒ–{model_type}æ¨¡å‹")
        return False
    
    if not model.is_available():
        print(f"âš ï¸ {model_type}æ¨¡å‹ä¸å¯ç”¨")
        return False
    
    print(f"âœ… {model_type}æ¨¡å‹å¯ç”¨: {model.model_name}")
    return True
```

#### è°ƒè¯•å·¥å…·

```python
def debug_claude_request(system_prompt, user_content, model=None):
    """è°ƒè¯•Claudeè¯·æ±‚"""
    if not model:
        model = model_factory.get_model("claude")
    
    print("ğŸ” è¯·æ±‚è¯¦æƒ…:")
    print(f"ç³»ç»Ÿæç¤ºé•¿åº¦: {len(system_prompt)}")
    print(f"ç”¨æˆ·å†…å®¹é•¿åº¦: {len(user_content)}")
    print(f"æ¸©åº¦è®¾ç½®: 0.7")
    print(f"æœ€å¤§ä»¤ç‰Œæ•°: 1024")
    
    try:
        start_time = time.time()
        response = model.generate_response(
            system_prompt=system_prompt,
            user_content=user_content,
            temperature=0.7,
            max_tokens=1024
        )
        end_time = time.time()
        
        print(f"âœ… è¯·æ±‚æˆåŠŸï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
        print(f"ç”Ÿæˆä»¤ç‰Œæ•°: {len(response.content.split())}")
        print(f"å“åº”å†…å®¹: {response.content[:200]}...")
        
        return response.content
        
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None
```

### æ€§èƒ½ç›‘æ§

```python
class ClaudeMonitor:
    def __init__(self):
        self.metrics = {
            'requests_total': 0,
            'requests_success': 0,
            'requests_failed': 0,
            'average_latency': 0,
            'error_rates': {}
        }
    
    def record_request(self, success, latency=None, error_type=None):
        """è®°å½•è¯·æ±‚æŒ‡æ ‡"""
        self.metrics['requests_total'] += 1
        
        if success:
            self.metrics['requests_success'] += 1
            if latency:
                self.metrics['average_latency'] = (
                    (self.metrics['average_latency'] * (self.metrics['requests_success'] - 1) + latency)
                    / self.metrics['requests_success']
                )
        else:
            self.metrics['requests_failed'] += 1
            if error_type:
                self.metrics['error_rates'][error_type] = \
                    self.metrics['error_rates'].get(error_type, 0) + 1
    
    def get_report(self):
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        total = self.metrics['requests_total']
        success_rate = (self.metrics['requests_success'] / total * 100) if total > 0 else 0
        
        return {
            'æ€»è¯·æ±‚æ•°': total,
            'æˆåŠŸç‡': f"{success_rate:.2f}%",
            'å¹³å‡å»¶è¿Ÿ': f"{self.metrics['average_latency']:.2f}ç§’",
            'é”™è¯¯åˆ†å¸ƒ': self.metrics['error_rates']
        }
```

**ç« èŠ‚æ¥æº**
- [model_factory.py](file://src/models/model_factory.py#L150-L200)
- [claude_model.py](file://src/models/claude_model.py#L60-L75)

## ç»“è®º

é€šè¿‡æœ¬æ•™ç¨‹ï¼Œæ‚¨å·²ç»æŒæ¡äº†åœ¨Moon Dev AIä»£ç†ç³»ç»Ÿä¸­ä½¿ç”¨Claudeæ¨¡å‹çš„å®Œæ•´æŠ€èƒ½ã€‚ä»åŸºç¡€çš„ModelFactoryåˆå§‹åŒ–åˆ°é«˜çº§çš„å¤šè½®å¯¹è¯ç®¡ç†ï¼Œä»åŒæ­¥è°ƒç”¨åˆ°å¼‚æ­¥å¤„ç†ï¼Œä»ç®€å•é—®ç­”åˆ°å¤æ‚åˆ†æï¼ŒClaudeæ¨¡å‹éƒ½èƒ½èƒœä»»å„ç§AIä»»åŠ¡ã€‚

å…³é”®è¦ç‚¹æ€»ç»“ï¼š

1. **ç»Ÿä¸€æ¥å£**ï¼šModelFactoryæä¾›äº†ç®€æ´ä¸€è‡´çš„æ¨¡å‹è·å–æ–¹å¼
2. **çµæ´»é…ç½®**ï¼šæ”¯æŒå¤šç§æ¨¡å‹ç‰ˆæœ¬å’Œå‚æ•°è°ƒæ•´
3. **ä¸Šä¸‹æ–‡ç®¡ç†**ï¼šæœ‰æ•ˆå¤„ç†å¤šè½®å¯¹è¯å’Œé•¿æ–‡æœ¬ç”Ÿæˆ
4. **ç³»ç»Ÿé›†æˆ**ï¼šæ— ç¼èå…¥ç°æœ‰çš„AIä»£ç†ç”Ÿæ€ç³»ç»Ÿ
5. **æœ€ä½³å®è·µ**ï¼šéµå¾ªæ€§èƒ½ä¼˜åŒ–ã€å®‰å…¨åˆè§„å’Œæ•…éšœæ’é™¤åŸåˆ™

éšç€AIæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒClaudeæ¨¡å‹å°†åœ¨æ›´å¤šåœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚å»ºè®®æŒç»­å…³æ³¨æ¨¡å‹æ›´æ–°ï¼Œä¼˜åŒ–æ‚¨çš„åº”ç”¨åœºæ™¯ï¼Œå¹¶ç§¯æå‚ä¸ç¤¾åŒºäº¤æµï¼Œå…±åŒæ¨åŠ¨AIæŠ€æœ¯çš„å‘å±•ã€‚